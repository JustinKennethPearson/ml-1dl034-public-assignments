{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMkQzOy36GPl"
   },
   "source": [
    "# Practical Lab 4\n",
    "So far, we have described various methods and libraries more or less in isolation on traditional datasets. In this lab, we will try to bring all that you have learned in the previous practical labs into one, as well as demonstrate important aspects of how you train machine learning models on real-world projects, investigate and pre-process your data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYKEDHAW6GPn"
   },
   "source": [
    "We will start by downloading a dataset about cars. The dataset contains six columns with some information regarding the cars, and one column with the acceptability. The information that is given to us is:\n",
    "\n",
    "- **The buying price**: very high (*vhigh*), high (*high*), medium (*med*), low (*low*)\n",
    "- **The price of the maintenance**: very high (*vhigh*), high (*high*), medium (*med*), low (*low*)\n",
    "- **Number of doors**: 2 (*2*), 3 (*3*), 4 (*4*), 5 or more (*5more*)\n",
    "- **Capacity in terms of persons to carry**: 2 (*2*), 4 (*4*), more than 4 (*more*)\n",
    "- **The size of luggage boot**: small (*small*), medium (*med*), big (*big*)\n",
    "- **Estimated safety of the car**: low (*low*), medium (*med*), high (*high*)\n",
    "\n",
    "Each car will also be given one out of four **acceptability class values**: unacceptable (*unacc*), acceptable (*acc*), good (*good*), very good (*vgood*).\n",
    "\n",
    "In this lab we will train a classifier on this dataset and find the best model to fit the data, i.e. we are going to classify if people want to buy the car based on the features or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and info found at the website\n",
    "# https://archive.ics.uci.edu/ml/datasets/car+evaluation\n",
    "\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/MarkusSagen/ML-datasets/master/cars_data.csv'\n",
    "urllib.request.urlretrieve(url, 'cars_data.csv')\n",
    "df = pd.read_csv('cars_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXGNFZoMOVwn"
   },
   "source": [
    "Utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRlPgsZ-6GPo"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJG2aD9N6GPo",
    "outputId": "b05c5e62-1496-4c6e-f66c-3dd2d5c4911d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Helper to print with prettier colors\n",
    "class c:\n",
    "    PURPLE = '\\033[95m'\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    END = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    \n",
    "print(f\"Printing {c.PURPLE}with {c.GREEN}pretty {c.FAIL}colors{c.END}{c.BOLD}!{c.END}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1s-ECKbWjcL"
   },
   "source": [
    "Inspect the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "PjtGwrM56GPq",
    "outputId": "3681c111-992f-41b1-ace1-f8759a5ae23c"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiu-5CCL6GPq",
    "outputId": "8ad462db-2f7c-49e2-b910-1b6d1e1bfcde"
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vbr7qOr26GPr"
   },
   "source": [
    "### Renaming the columns of our dataset \n",
    "Using the information about the dataset we gathered from the original website, we can now rename the columns of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rhW59g256GPs",
    "outputId": "6c0e7c30-597e-4607-d16b-0068214443ff"
   },
   "outputs": [],
   "source": [
    "\n",
    "column_names = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"]\n",
    "\n",
    "df.columns = column_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0m4-CNYE6GPs"
   },
   "source": [
    "### Preprocess and formating your data\n",
    "Something we have touched upon a bit in previous labs is the importance of cleaning and transforming the data in such a way that it is easier for you and the machine learning models to work with.    \n",
    "\n",
    "We have actually cheated a bit... The dataset you have received has been modified to include some missing values. This is common in real-world datasets and something you need to deal with first! Machine learning models don't know how to fit a missing value, and it is therefore your job as a machine learning expert to choose one of several ways of how to deal with missing data ;-)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuVZezuj6GPt"
   },
   "source": [
    "By printing out the information for each column, we can compare quickly if any of the columns contain missing values. Please note that the features such as `.head()`, `.info()`, etc. are features built into the Pandas library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGhDB8Mb6GPt",
    "outputId": "1de7f7ed-f008-4e98-a8b6-2ea78a35fb4d"
   },
   "outputs": [],
   "source": [
    "nan_values_per_feature = df.isnull().sum()\n",
    "nan_total = sum(list(df.isnull().sum()))\n",
    "\n",
    "print(f\"The dataset length: \\t\\t{c.BLUE}{len(df)}{c.END}\")\n",
    "print(f\"Total number of missing values: {c.BOLD}{nan_total}{c.END}\\n\")\n",
    "\n",
    "print(f\"{c.BOLD}Printing how many entries in each column contain no NaN values{c.END}:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma_fTGxr6GPt"
   },
   "source": [
    "#### ASSIGNMENT a)\n",
    "Remove or handle the missing values in the dataset in an appropriate way.\n",
    "- **HINT**: Pandas have multiple built-in methods for dealing with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "kQ7avawV6GPt",
    "outputId": "6ce7af0e-be71-4c67-f5b9-ea3216f94d0d"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHDWiho46GPu"
   },
   "source": [
    "Let's inspect the dataset a bit more. What kind of value does each column contain?\n",
    "Let's print the content of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Li9TfsYM6GPu",
    "outputId": "f7b2b1ac-a64f-4a9d-b9fb-c317c63b3590"
   },
   "outputs": [],
   "source": [
    "print(\"List all attributes in the dataset and count how many entries of each kind\\n\")\n",
    "for col in column_names:\n",
    "    print(f\"{c.BOLD}{col}:{c.END}\")\n",
    "    print(f\"{df[col].value_counts()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTWVJkLW6GPu"
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joXXTTG36GPu"
   },
   "source": [
    "From this we can observe several things:\n",
    "- There is a very even distribution of attributes in all categories.\n",
    "- All categories consist of categorical attributes (since no feature has only numbers).\n",
    "- Even the number of doors is a categorical value since it is grouped into having \"_5orMore_\" as a possible attribute.\n",
    "- Since ML models don't know how to order categorical values such as \"_small_\", \"_medium_\", \"_big_\", we first need to convert the categorical values into numerical representations. This will ensure that the machine learning model knows how to order and compare the different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBycVoZj6GPv"
   },
   "source": [
    "#### Assignment b)\n",
    "Convert all the features in the dataset from categorical values into ordinal data/numerical data.\n",
    "- There are multiple ways of doing this both with Pandas, Scikit-learn, or even external libraries dedicated to doing just this sort of thing.\n",
    "- Search online for methods for converting categorical data into numerical or ordinal data.\n",
    "- Do not spend too much time on this. Make it **SIMPLE!**.\n",
    "- Your goal is to classify the data and all these pre-processing methods are steps to get you towards that goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rp6uWZZx6GPv",
    "outputId": "9ad6724a-4cb4-4ea0-eab4-8989c5ca51fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype as is_num\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CODE FOR CHECKING THE FORATING OF THE DATASET\n",
    "assert is_num(df['buying']),   \"The 'buying' column contained categorical values\"\n",
    "assert is_num(df['maint']),    \"The 'maint' column contained categorical values\"\n",
    "assert is_num(df['doors']),    \"The 'doors' column contained categorical values\"\n",
    "assert is_num(df['persons']),  \"The 'persons' column contained categorical values\"\n",
    "assert is_num(df['lug_boot']), \"The 'lug_boot' column contained categorical values\"\n",
    "assert is_num(df['safety']),   \"The 'safety' column contained categorical values\"\n",
    "assert is_num(df['class']),    \"The 'class' column contained categorical values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nR-3y0f6GPw"
   },
   "source": [
    "Once the data is finally numeric, we can do things like plot the distribution of the data, draw a histogram, or similar. In some steps above, we sprinted how many attributes we had in each column, but now we can instead inspect it visually. A very handy library for making more complex and comparative plots is a library called [`seaborn`](https://seaborn.pydata.org/examples/index.html). We know this will be a very even distribution among the features, but it is good practice to do, especially when you are working with more complex datasets!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-kxdDQcn6GPw",
    "outputId": "073c2ab3-c28e-4aa8-cdf8-fb6a85a775d6"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLP1UyhR6GPx"
   },
   "source": [
    "### Finishing up on the pre-processing\n",
    "\n",
    "The final step of the pre-processing steps is to split the dataset into features $X$ and label $y$ - Meaning all the features we want the model to use when training is in X, and the label, what the model tries to predict, is filtered out and used in a separate dataset. If we do not remove the labels for the dataset, the model will learn to only look at those features and not actually learn.   \n",
    "  \n",
    "Once we have the data divided into features and labels, we split each of them into a `training` and `testing` set before training and evaluating our model.  \n",
    "  \n",
    "Note that there are many pre-processing and feature engineering steps that we did not cover which one could do like One-Hot Encoding. For regression problems, for instance, it is common to normalize the data - making it more stable to variance between the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlTsJh7H6GPx"
   },
   "source": [
    "#### Assignment c)\n",
    "- Divide the dataset into two datasets, one containing only the features and one containing only the labels you later want to predict\n",
    "- Split the features and labels into training and testing sets. Choose a split you think seems reasonable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-G81SHC6GPx"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gF0lPvsV6GPx"
   },
   "source": [
    "## Training a simple classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI-O9AwN6GPy"
   },
   "source": [
    "Let's create a simple classifier to predict if people want to buy a car or not.  \n",
    "We will start by using a Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMdqCIKZ6GPy"
   },
   "source": [
    "#### Assignment d)\n",
    "- Create a Logistic Regression model similar to what you did in practical lab3.\n",
    "- Train your model on the training data set you just created and evaluate it against the test dataset you created in the section above.\n",
    "- Compute the accuracy between your predicted labels and the actual labels.\n",
    "- **HINT**: If you set your training or testing dataset in the above section with an improper division (too much data in either dataset), then the test below will fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5-pZbQk6GPy",
    "outputId": "80a10338-66eb-417a-bc4b-b4e02022ef1d"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# CODE PROVIDED TO TEST YOUR MODEL\n",
    "assert 0.7 <= acc <= 0.95, \"Your train/split division have too much data in the train or test set\\\n",
    "                            \\nTry to change the division to create a more balanced model\"\n",
    "\n",
    "print(f\"The trained model has an accuracy of: {100*acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUfvy3QH6GPy"
   },
   "source": [
    "Not bad at all! But can we do better???   \n",
    "The performance of the model depends on several things, such as the choice of model, its hyperparameters, the random seed used when initializing training, the train/split ratio when partitioning the datasets, etc.   \n",
    "\n",
    "Choosing between all possible parameters that could improve the model is a vast topic all of itself called **Hyper-parameter tuning**. We will not discuss it more here in this lab, only mention that it exists and that two commonly used techniques for finding the optimal hyperparameters are *Grid search* and *Bayesian optimization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ucrqur8f6GPz"
   },
   "source": [
    "## Cross-validation (CV) \n",
    "\n",
    "\n",
    "To prevent overfitting when evaluating different methods and settings one can  partition the available data into three sets instead; training, validation, and test set. But this will drastically reduce the number of samples which can be used for learning the model. A solution to this problem is a procedure called Cross-validation (CV), where a validation set no longer is needed.\n",
    "\n",
    "You can read more about CV [here](https://scikit-learn.org/stable/modules/cross_validation.html). (And we strongly recommend you to read about this for a better understanding).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V79s43Az6GPz"
   },
   "source": [
    "#### ASSIGNMENT e)\n",
    "We will now look into some other classifiers, and try them against each other using CV. Investigate the five classifiers\n",
    "SVC ([Guide](https://scikit-learn.org/stable/modules/svm.html#svm-classification), [code](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)), \n",
    "SVC-linear ([Guide](https://scikit-learn.org/stable/modules/svm.html#svm-classification), [code](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)), \n",
    "decision trees ([Guide](https://scikit-learn.org/stable/modules/tree.html#tree), [code](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)), \n",
    "k-NN ([Guide](https://scikit-learn.org/stable/modules/neighbors.html#classification), [code](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)), \n",
    "and random forest ([Guide](https://scikit-learn.org/stable/modules/ensemble.html#forest), [code](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)), \n",
    "and choose at least three of them for this exercise. Run them against each other using CV (*with for example 10 runs*), and evaluate the performance by finding the average accuracy for each model. Print the result for each models average performance and determine which model you would choose, based on this.\n",
    "When you run these models, make sure that you pass in `random_state=42` as an argument, or some other number as your random state to ensure consistent training when you try to re-run the experiment.\n",
    "\n",
    "You are encouraged to explore [more algorithms](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) as well.\n",
    "\n",
    "<br>\n",
    "\n",
    "**HINT**: When using cross-validation you do not need to make a train/test-split, since it does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vb6MUbWb6GPz",
    "outputId": "b29058e8-fd11-4b54-f447-00f84af9c6e7"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4NJZ0_B6GPz"
   },
   "source": [
    "We have now seen how cross-validation can be used to more fairly compare the overall performance between different models. However, cross-validation can also be used when testing different hyper-parameters to find, on average, which hyper-parameters creates the best model with a trade-off between over- and underfitting?  \n",
    "  \n",
    "We provide some optional assignments for those who wish to investigate it further, but, again, it is __not__ mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juoeYYph6GP0"
   },
   "source": [
    "## Interpreting a model and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DSTOCwR6GP0"
   },
   "source": [
    "Up until now we have used all the features of the model to make our predictions. However, machine learning models will try to use all the available features it has from the dataset to best fit the model, even if some features are not needed. This can lead to the model performing worse when using all the features, compared to a handful of them. We will demonstrate this in the final part of the notebook.   \n",
    "  \n",
    "Machine learning models often require vast amounts of data and perform worse the less training data it has. We will illustrate both these cases in the final part of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUzQRj-w6GP0"
   },
   "source": [
    "#### ASSIGNMENT f)\n",
    "- Train and fit a Random Forest classifier using the train/test split of (20/80) for the training and testing - meaning it uses very little data for its training.\n",
    "\n",
    "Generally the more data it has to train on, the better it performs and the more features to use, the more complex and reliant on a vast amount of data the model becomes.\n",
    "\n",
    "<br>\n",
    "We will use the Random Forest classifier you trained here to showcase how one can view the feature importance when clearing a classifier. We will also use it to showcase how we can remove certain features, build a new classifier and potentially get even better accuracy by removing certain features. This is especially true when you have a lot of features or few data samples available for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64SUc1Gk6GP0",
    "outputId": "52d89b62-37dd-4159-c6db-6b02c07f2fdb"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#\n",
    "#\n",
    "\n",
    "# CODE PROVIDED TO TEST YOUR MODEL\n",
    "assert 0.9 <= acc <= 0.96, \"The train/test split should be (20/80)\"\n",
    "print(f\"The model accuracy is: {c.BOLD}{acc:.2f}{c.END}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXvKdD-R6GP0",
    "outputId": "5611075d-6c9e-4fd3-e263-3f3969f605ae"
   },
   "outputs": [],
   "source": [
    "feature_scores = pd.Series(clf.feature_importances_, index = X_train.columns).sort_values(ascending = False)\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5UsWHf56GP0"
   },
   "source": [
    "Set a nice color palette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "Us2XhAD26GP0",
    "outputId": "cbbdfc21-0d95-492c-a5c1-f7d41421ea1b"
   },
   "outputs": [],
   "source": [
    "# We can choose different color schemes when plotting\n",
    "import seaborn as sns\n",
    "sns.palplot(sns.color_palette(\"deep\"))\n",
    "sns.palplot(sns.color_palette(\"BuGn_r\", 10)) \n",
    "sns.palplot(sns.color_palette(\"pastel\"))\n",
    "\n",
    "# Set the color palette we want to use\n",
    "sns.set_palette('BuGn_r', n_colors=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekLtV-H36GP1"
   },
   "source": [
    "##### Plot the importance of each feature according to our trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "5WM8y9SA6GP1",
    "outputId": "22ca23fb-787c-4563-d96a-51a8ca6a8a22"
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=feature_scores, y=feature_scores.index, )\n",
    "plt.title(\"Importance of each Feature for the Random Forest classifier\")\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtVAi8406GP1"
   },
   "source": [
    "From this we can conclude that both the feature `doors` and `lug_boot` does not seem too important for the classifier.  \n",
    "\n",
    "We therefore try to create a new classifier with a dataset where we have removed one of those features, we can start by removing the `doors` feature, but feel free to remove the other feature or even both of them and see what happens to the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEizV7K56GP1",
    "outputId": "d9f48862-7e7e-40ae-bc92-11d33225a7b8"
   },
   "outputs": [],
   "source": [
    "X_train_new = X_train.drop(['doors'], axis=1)\n",
    "X_test_new = X_test.drop(['doors'], axis=1)\n",
    "\n",
    "\n",
    "clf.fit(X_train_new, y_train)\n",
    "pred = clf.predict(X_test_new)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(f\"The accuracy after dropping one of the feature is now: {c.BOLD}{acc:.2f}{c.END}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUOSYPDh6GP1"
   },
   "source": [
    "We can see that the performance increased from about 0.93 to about 0.94 by only removing one of the least significant features.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7oMIZEuMvx1"
   },
   "source": [
    "We will plot the confusion matrix for our newly created classifier to get some insights on which labels the models performs well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "car_condition = ['unacc', 'accept', 'good', 'very good'] # Labels of the target variable\n",
    "\n",
    "conf_matrix = confusion_matrix(pred, y_test); # Set to None to show the actual numbers\n",
    "\n",
    "# Display an aesthetic confusion matrix with colours:\n",
    "sns.set(font_scale = 1)\n",
    "sns.heatmap(conf_matrix, annot = True, cmap=\"YlGnBu\", fmt = 'd', xticklabels = car_condition, yticklabels = car_condition)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is used to define the performance of a classification algorithm. It is used to evaluate the accuracy of a classifier. It is a table with 4 different combinations of predicted and actual values. The number of correct and incorrect predictions are summarized with count values and broken down by each class.\n",
    "\n",
    "The entries in the matrix are the number of observations that fall into each combination of predicted and actual values. The diagonal entries represent the number of observations that were correctly classified, while the off-diagonal entries represent the number of observations that were incorrectly classified.\n",
    "\n",
    "The Confusion matrix allows us to calculate several important evaluation metrics such as accuracy, precision, recall and F1-score.\n",
    "\n",
    "In the case of multi-class classification, the confusion matrix will have nxn dimensions, where n is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLs6oKuDM-wf"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "You are now done with the lab's mandatory parts, but feel free to continue with the optional assignments. \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "--------------------------------------\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aR6c52yp6GP2"
   },
   "source": [
    "## (Optional)\n",
    "This optional part of the assignment will highlight how one can use hyper-parameter optimization to find the best number of estimators to use in a Random Forest classifier. There are multiple ways to do this, such as [Random Search](https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html), [Grid Search](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html), [More Grid Search](https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/), and [Even More on Grid Search](https://sklearn-evaluation.readthedocs.io/en/stable/user_guide/grid_search.html). You can even do your own for-loop with the parameters you want to test against and then evaluate the model using those parameters.\n",
    "\n",
    "Your goal is to find the optimal number of estimators for a Random Forest classifier so that it will yield the best performance possible. Along the way, we will plot the classifier's average performance for the different number of estimators we have chosen to use. To accurately depict this, we run our model using cross-validation with k=10 folds.  \n",
    "\n",
    "- Try to train a Random Forest classifier with multiple different n_estimators (Ex. all choices of `n_estimators` between 1 and 100) using a CV of 10 runs. \n",
    "- Compute the mean and standard deviation from running these evaluations with cross-validation(CV).\n",
    "- Plot the mean of the model performance (the y-axis) in a graph below against the number of n_estimators (the x-axis).\n",
    "- If you choose to plot the standard deviation along with the mean performance, you may want to scale the standard deviation down by dividing it with a number between 10 to 20. Otherwise, you may only see the fluctuation of the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rO9Cg9uq6GP2",
    "outputId": "ac18d0d0-fb42-4db5-f2d2-ca822b719e0d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# To use the provided code below you need to have written the lists/arrays:\n",
    "# estimators_list:  List of how many estimators to use in the RandomForest model\n",
    "# means:       List of the mean values for the correponding number of estimators\n",
    "# stds:         List of the std values for the correponding number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "CQFpyiwc6GP2",
    "outputId": "63b63e65-c8f2-4219-d023-d823d7c0aed9"
   },
   "outputs": [],
   "source": [
    "# CODE PROVIDED FOR YOU TO PLOT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "means = np.array(means)\n",
    "stds = np.array(stds)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xlim([-1, 101])\n",
    "plt.ylim([0.83, 0.9])\n",
    "plt.xlabel(\"Number of estimators\")\n",
    "plt.ylabel(\"Average Performance\")\n",
    "\n",
    "plt.title(\"Plot of n_estimators for a RF classifier after CV=10\")\n",
    "plt.plot(estimators_list, means, label = \"Performance per n_estimators\")\n",
    "plt.fill_between(estimators_list, y1 = means+stds/10, y2 = means-stds/10, alpha = 0.2) # Deviation bands\n",
    "   \n",
    "best_nr_estimators_idx = np.argmax(means)\n",
    "best_nr_estimators = estimators_list[best_nr_estimators_idx]\n",
    "\n",
    "# Plot dotted lines\n",
    "plt.hlines(y = np.max(means), xmin = -5, xmax = best_nr_estimators, color = 'black', linestyles= 'dashed') \n",
    "plt.vlines(x = best_nr_estimators, ymin = 0, ymax = np.max(means), color = 'black', linestyles= 'dashed') \n",
    "\n",
    "plt.plot(best_nr_estimators, np.max(means), color='red', marker='*', linewidth=1, markersize=16, label=f\"Optimal n_estimators={best_nr_estimators}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJ1YzLfi6GP2"
   },
   "source": [
    "## (Optional)\n",
    "In this optional section, we want to investigate an important topic - that of **calibration**.  \n",
    "If a machine learning model is trained to predict data to belong to either label 0 or 1. We say that the model is **well-calibrated** if the model predicts the data to belong to label 1 80% of the time if 80% of the data correctly should be classified as belonging to label 1.   \n",
    "  \n",
    "This is very important in the context of self-driving cars, medicine, or other instances where machine learning models must make crucial decisions. In those instances, if the model predicts an outcome to be dangerous with 90% probability, then we want it to correspond to the true likelihood of that being dangerous.  \n",
    "  \n",
    "To investigate this, we will first need to join labels into two categories instead of the current four. Looking at the original labels of the dataset (before we converted them into numerical values), we had the labels \"unacceptable\", \"acceptable\", \"good\", and \"very good\". We will reformulate it into a binary classification problem by assuming that all cars with the label \"acceptable\" and above indicates that the car will be sold.   \n",
    "  \n",
    "YOUR TASK:  \n",
    "- Convert the labels into two labels, where all labels of \"acc\" or above are deemed to belong to label 1 and the rest to label 0. \n",
    "- Split the dataset into training and testing dataset with their respective labels,\n",
    "- Define several different classifier models you want to test,\n",
    "- Store each classifier in a list called `classifier_list` of tuples with the classifier and the name of the classifier:\n",
    "    - **Ex**: \n",
    "    classifier_list = [(cl1, \"classifier 1\"),  (cl2, \"classifier 2\"), ... ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RACF3kNi6GP3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of all classifiers as tuple with classifier and name\n",
    "#classifier_list = [ ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "id": "unT5pmKjI_dp",
    "outputId": "b0ea7392-2399-4e07-e24f-f1049509fc5d"
   },
   "outputs": [],
   "source": [
    "# CODE PROVIDED FOR YOU TO PLOT THE DIFFERNT CLASSIFIERS\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Set a prettier plotting format\n",
    "plt.style.use(\"bmh\")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k--\", label=\"Ideally Calibrated Model\")\n",
    "for clf, name in classifier_list:\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prob = clf.predict_proba(X_test)[:, 1]\n",
    "    else: \n",
    "        prob = clf.decision_function(X_test)\n",
    "        prob = (prob - prob.min()) / (prob.max() - prob.min()) # Normalize to percentages\n",
    "        \n",
    "    fraction_of_positives, mean_pred_value = calibration_curve(y_test, prob, n_bins=10)\n",
    "\n",
    "    # Reliability / Calibration plot\n",
    "    ax1.plot(mean_pred_value, fraction_of_positives, \"o-\", label=\"%s\" % (name, ))\n",
    "    \n",
    "    # Histogram plot\n",
    "    ax2.hist(prob, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n",
    "\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration/Reliability Curve')\n",
    "\n",
    "ax2.set_title(\"Histogram over Predicted Values - Only 0 or 1's for Ideal Classifier\")\n",
    "ax2.set_xlabel(\"Mean predicted value\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PracticalNotebook4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
