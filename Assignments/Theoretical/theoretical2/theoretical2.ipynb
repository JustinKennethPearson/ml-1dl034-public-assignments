{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet classification with naive bayes\n",
    "\n",
    "For this notebook we are going to implement a naive bayes classifier for classifying tweets about Trump or Obama based on the words in the tweet. Recall that for two events A and B the bayes theorem says\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "where P(A) and P(B) is the ***class probabilities*** and P(B|A) is called ***conditional probabilities***. this gives us the probability of A happening, given that B has occurred. So as an example if we want to find the probability of \"is this a tweet about Trump given that it contains the word \"president\" \" we will obtain the following \n",
    "\n",
    "$$ P(\\text{\"Trump\"}|\\text{\"president\" in tweet}) = \\frac{P(\"\\text{\"president\" in tweet}|\\text{\"Trump\"})P(\\text{\"Trump\"})}{P(\"\\text{\"president\" in tweet})} $$\n",
    "\n",
    "This means that to find the probability of \"is this a tweet about Trump given that it contains the word \"president\" \" we need the probability of \"president\" being in a tweet about Trump, the probability of a tweet being about Trump and the probability of \"president\" being in a tweet. \n",
    "\n",
    "Similarly if we want to obtain the opposite \"is this a tweet about Obama given that it contains the word \"president\" \"\n",
    "we get \n",
    "\n",
    "$$ P(\\text{\"Obama\"}|\\text{\"president\" in tweet}) = \\frac{P(\\text{\"president\" in tweet}|\\text{\"Obama\"})P(\\text{\"Obama\"})}{P(\\text{\"president\" in tweet})} $$\n",
    "\n",
    "where we need the probability of \"president\" being in a tweet about Obama, the probability of a tweet being about Obama and the probability of \"president\" being in a tweet. \n",
    "\n",
    "We can now build a classifier where we compare those two probabilities and whichever is the larger one it's classified as \n",
    "\n",
    "if P(\"Trump\"|\"president\" in tweet) $>$ P(\"Obama\"|\"president\" in tweet)\n",
    "    \n",
    "   Tweet is about Trump\n",
    "\n",
    "else\n",
    "   \n",
    "   Tweet is about Obama\n",
    "\n",
    "Now let's expand this to handle multiple features and put the Naive assumption into bayes theroem. This means that if features are independent we have \n",
    "\n",
    "$$ P(A,B) = P(A)P(B) $$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{P(b_1|A)P(b_2|A)...P(b_n|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{\\prod_i^nP(b_i|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "\n",
    "So with our previous example expanded with more words \"is this a tweet about Trump given that it contains the word \"president\" and \"America\" \" gives us \n",
    "\n",
    "$$ P(\\text{\"Trump\"}|\\text{\"president\", \"America\" in tweet}) = \\frac{P(\\text{\"president\" in tweet}|\\text{\"Trump\"})P(\\text{\"America\" in tweet}|\\text{\"Trump\"})P(\\text{\"Trump\"})}{P(\\text{\"president\" in tweet})P(\\text{\"America\" in tweet})} $$\n",
    "\n",
    "As you can see the denominator remains constant which means we can remove it and the final classifier end up\n",
    "\n",
    "$$y = argmax_A P(A)\\prod_i^nP(b_i|A) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:33:05.384466Z",
     "start_time": "2021-01-21T13:33:05.376779Z"
    }
   },
   "outputs": [],
   "source": [
    "#stuff to import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:33:05.634904Z",
     "start_time": "2021-01-21T13:33:05.628467Z"
    }
   },
   "outputs": [],
   "source": [
    "assert pd.__version__ == \"1.2.1\", \"Looks like you don't have the same version of pandas as us!\"\n",
    "assert np.__version__ == \"1.19.4\", \"Looks like you don't have the same version of numpy as us!\"\n",
    "assert sklearn.__version__ == \"0.24.0\", \"Looks like you don't have the same version of sklearn as us!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:30:18.885001Z",
     "start_time": "2021-01-21T13:30:18.675615Z"
    }
   },
   "outputs": [],
   "source": [
    "df_t = pd.read_csv('trump_20200530.csv')\n",
    "trump_tweets = df_t['text']\n",
    "df_t = pd.read_csv('Tweets-BarackObama.csv')\n",
    "obama_tweets = df_t['Tweet-text']\n",
    "\n",
    "tweet_data = trump_tweets.append(obama_tweets, ignore_index=True)\n",
    "tweet_labels = np.array(['T' for _ in range(len(trump_tweets))] + ['O' for _ in range(len(obama_tweets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:30:20.197890Z",
     "start_time": "2021-01-21T13:30:20.181611Z"
    }
   },
   "outputs": [],
   "source": [
    "lab, counts = np.unique(tweet_labels, return_counts=True)\n",
    "print('Number of tweets about ', lab[0], ': ', counts[0])\n",
    "print('Number of tweets about ', lab[1], ': ', counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have many more Trump than Obama Tweets so simlpy guessing that a tweet is a Trump tweet already gives you a classifier that is correct about 70% of the time, but we can do better than this.\n",
    "\n",
    "Now lets split the data into a training set and a test set using scikit-learns train_test_split function \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:30:22.469848Z",
     "start_time": "2021-01-21T13:30:22.450611Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split data into train_tweets, test_tweets, train_labels and test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to build our classifier is \"probability of tweet about Obama\" P(O) , \"probability of tweet about Trump\" P(T), \"probability of word in tweet given tweet about Obama\" P(w|O) and \"probability of word in tweet given tweet about Trump\" P(w|T). Start by calculating the probability that a tweet is about Obama and trump respectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_O = ...\n",
    "P_T = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For P(w|O), P(w|T) we need to count how many tweets each word occur in. Count the number of tweets each word occurs in and store in the word counter. An entry in the word counter is for instance  {'president': 'O':87, 'T': 100} meaning president occurs in 87 words about Obaman and 100 tweets about Trump. Be aware that we are not interested in calculating multiple occurances of the same word in the same tweet.  For each word convert it to lower case. You can use Python's [lower](https://www.w3schools.com/python/ref_string_lower.asp). Another handy Python string method is [split](https://www.w3schools.com/python/ref_string_split.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = {}\n",
    "for (tweet, label) in zip(train_data, train_labels):\n",
    "# ... Count number of tweets each word occurs in and store in word_counter where an entry looks like ex. {'word': 'O':98, 'T':10}\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets work with a smaller subset of words. Find the 100 most occuring words in tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_words_to_use = 100\n",
    "popular_words = sorted(word_counter.items(), key=lambda x: x[1]['O'] + x[1]['T'], reverse=True)\n",
    "popular_words = [x[0] for x in popular_words[:nr_of_words_to_use]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute P(w|O), P(w|T) for the popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_w_given_t = {}\n",
    "P_w_given_o = {}\n",
    "for word in popular_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = {\n",
    "    'basis'  : popular_words,\n",
    "    'P(T)'   : P_O,\n",
    "    'P(O)'   : P_T,\n",
    "    'P(w|O)' : P_w_given_o,\n",
    "    'P(w|T)' : P_w_given_t\n",
    "    }   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tweet_classifier function that takes your trained classifier and a tweet and returns wether it's about Trump or Obama unsing the popular words selected. Note that if there are words in the basis words in our classifier that are not in the tweet we have the opposite probabilities i.e P(w_1 occurs )*  P(w_2 does not occur) * .... if w_1 occurs and w_2 does not occur. The function should return wether the tweet is about Obama or Trump i.e 'T' or 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_classifier(tweet, classifier_dict):\n",
    "    \"\"\" param tweet: string containing tweet message\n",
    "        param classifier: dict containing 'basis' - training words\n",
    "                                          'P(T)' - class probabilities\n",
    "                                          'P(O)' - class probabilities\n",
    "                                          'P(w|O)' - conditional probabilities\n",
    "                                          'P(w|T)' - conditional probabilities\n",
    "        \n",
    "        return: either 'T' or 'O'\n",
    "    \"\"\"\n",
    "    words_in_tweet = np.unique([x.lower() for x in tweet.split()])\n",
    "    \n",
    "    # ... Code for classifying tweets using the naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier, test_tweets, test_labels):\n",
    "    total = len(test_tweets)\n",
    "    correct = 0\n",
    "    for (tweet,label) in zip(test_tweets, test_labels):\n",
    "        predicted = tweet_classifier(tweet,classifier)\n",
    "        if predicted == label:\n",
    "            correct = correct + 1\n",
    "    return(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_classifier(classifier, test_tweets, test_labels)\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
