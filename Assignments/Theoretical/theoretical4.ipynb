{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T09:36:17.061125Z",
     "start_time": "2021-02-09T09:36:16.076179Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import pandas as pd\n",
    "\n",
    "!pip install treelib\n",
    "from treelib import Node, Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the entropy\n",
    "\n",
    "Entropy is a measure of information, used in many fields, including physics, statistics and machine learning. In information theory, entropy represents the amount of uncertainty or surprise in a random variable or sequence. It quantifies the average amount of information produced by a stochastic process or received through a communication channel. High entropy implies higher unpredictability or randomness, while low entropy indicates more predictability or order. For instance, if Alice tosses a fair coin and records the result, how many bits of information does she need to send it to Bob so that he knows the result as well? He needs exactly one bit (0 = heads, 1 = tail) on average. We always assume they both know what the setup of the experiment is.\n",
    "\n",
    "The entropy is written $H(x)$ and is defined as:\n",
    "$H(x) = \\mathbb{E}[-log_2(x)] = - \\Sigma_{i=1}^N p(x_i) log_2(p(x_i))$.\n",
    "\n",
    "A bit of intuition\n",
    "\n",
    "**(1)** - As a measure of information, we want the entropy to be additive. If you get one e-mail, you get n bits of information, if you get two e-mails, you get n+m bits of information. Unfortunately, when two independant random variables are realized, they are multiplied together : $p(x_a \\cap x_b) = p(x_a)p(x_b)$. In order to make them additive, the logarithm function can be used: $\\log p(x_a \\cap x_b) = \\log p(x_a) + \\log p(x_b)$.\n",
    "\n",
    "**(2)** - The rarer the event, the more information must be transferred when it occurs. For instance, if a coin is rigged and always gives tail, Alice doesn't even need to send anything to Bob, since he already know that it will be tail. Remember that both Alice and Bob know the exact setup of the experiment. Newspaper tend to follow this trend as well: they don't talk about how the sun will rise this morning, talk a bit about the weather forecast (a few numbers) but might dedicate a full page to the new, shiney and overhyped tv-series dedicated to the life of Carl XVI Gustaf. So the amount of information is invertly proportional to the probability: $1/p(x)$.\n",
    "\n",
    "**(3)** - So far we have $log_2(1/(p(x)) = -log_2(p(x))$, and we take its expectation such that it gives the average information in a random variable. **Note that the base of the log is arbitrary, you will often see either $e$ or $2$ as a base**. I personnaly enjoy using $log_2$ because the entropy will be in bits which is easier to interpret (with $e$, the entropy will be in nats)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Entropy of the Normal distribution\n",
    "\n",
    "Calculate the (base 2) entropy of a Normal distribution.\n",
    "\n",
    "The pdf of the Normal distribution is: $\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$.\n",
    "You need to find: $\\mathbb{E}[-log_2(\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})]$\n",
    "Do it step-by-step, first the log (base 2), then flip the signs, then take the expectation of the remaining formula.\n",
    "\n",
    "The result will be: $\\frac{1}{2} log_2(2 \\pi e \\sigma^2)$\n",
    "\n",
    "Formulas to use (or not, as you want, who am I to dictate anything?):\n",
    "* $\\log_2(a*b) = \\log_2(a) + \\log_2(b)$\n",
    "* $\\log_2(x) = \\log(x)/\\log(2)$\n",
    "* $\\log_2(\\exp(u)) = u/\\log(2)$. Be careful!! The $\\log_2$ is no longer base 2 in the right hand side, but base $e$.\n",
    "* $\\mathbb{E}[ax + by] = a\\mathbb{E}[x] + b\\mathbb{E}[y]$. That means that all the constants go out of the expectation. In this case, both $\\mu$ and $\\sigma^2$ are constants. $x$ is not constant.\n",
    "* $\\mathbb{E}[x] = \\mu$ and $\\mathbb{V}[x] = \\sigma^2$.\n",
    "* $\\mathbb{E}[(x - \\mu)^2 / \\sigma^2] = \\mathbb{E}[(x - \\mu)^2] / \\sigma^2 = \\mathbb{V}[x] / \\sigma^2 = \\sigma^2/\\sigma^2 = 1$\n",
    "* $1/\\log(2) = \\log_2(2^{1/log(2)}) = \\log_2(e)$\n",
    "\n",
    "Now that you have found the entropy of a normal distribution we can wonder about the intuitive meaning of the thing. First, the entropy has only one variable: $\\sigma^2$. Let's imagine a friend of yours invented a new die that, when tossed gives you a random number sampled from a normal distribution (rounded to the nearest integer). He calls it the Normal Die ™️ (confusing name, I know, there's already a lot of hatred for the normal distribution being called that way). Like for a regular die, where you know the probability of each face, you are given the mean and the variance. If I buy one such die, and toss it, how many bits do I need to send to you for you to know which value is displayed on the die? The entropy formula gives you that, but:\n",
    "* If the mean is 0, 20 or -37, the entropy doesn't change. This is because you know the mean is common knowledge, and your friend only needs to send the deviation from the mean to you. If the mean is 20, he tosses the dice and get 24, but only needs to send 24-20=+4 (2 bits, plus a sign bit = 3 bits in total) to you. When you get the result: +4, you just have to add the mean to recover the true number. Remember that the mean and variance are common knowledge.\n",
    "* The bigger $\\sigma^2$, the bigger the variance and the more bits I need to send to you.\n",
    "This is what the formula above states, meaning entropy is also a measure of uncertainty. More variance means more bits to send, so a bigger entropy.\n",
    "\n",
    "Advanced note: you might notice that if the variance is 0, the log is negative ($- \\infty$) do not get confused, and just considered that negative entropies get clipped to 0 bits. This works because we rounded the values generated by the continuous normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "> Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy of a coin toss\n",
    "\n",
    "Let's imagine we can choose how to rig a coin so that we can choose its average probability of heads. That setup is exactly how a Bernoulli r.v. works. You choose its parameter $\\theta$ and then $p(heads) = \\theta$. Note that $\\theta \\in [0, 1]$ and for most coins: $\\theta = 0.5$.\n",
    "\n",
    "* Step 1: Find and write down the formula for the entropy of a bernoulli variable on the internet. Wikipedia is an excellent resource in statistics (and often reliable, but don't cite it still). Do not forget to change the $log$ into $log_2$!\n",
    "\n",
    "* Step 2: The formula involves $x * log_2(x)$. What happens when $x = 0$? We get $0 * -\\infty = I.F.$, so this is clearly not the right approach. The good way of finding out what happens is by applying [L'Hôpital's rule](https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule) and finding the limit of $x * log(x)$ when $x$ approaches 0. You can either do it as an exercise if you think it's fun (it is quick and rewarding in practice, give it a shot :), or you can look up the answer in the Wikipedia page above.\n",
    "\n",
    "* Step 3: Plot the entropy of a bernoulli r.v. below. Do not forget to use the right base for the log (log base 2 exists in numpy, look it up). And make it so that it works even when $p$ is 0 and 1.\n",
    "\n",
    "* Step 4: What can you conclude about the plot? Does it correspond to what is written up there (what is the entropy when the parameter is 0, 0.5, 1)? What parameter gives the highest entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "> Write your solution here\n",
    "\n",
    "**Step 1 and Step 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T09:36:45.395767Z",
     "start_time": "2021-02-09T09:36:45.224355Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Domain of the parameter p of a Bernoulli rv: p ∈ [0, 1]\n",
    "p = np.linspace(0, 1, 1000)\n",
    "\n",
    "def xln2x(x):\n",
    "    \"\"\" Computes x * log_2(x) in a safe way.\"\"\"\n",
    "    # If x is not a numpy array, it becomes one\n",
    "    if not isinstance(x, np.ndarray):\n",
    "        x = np.array(x)\n",
    "    # STUDENT CODE STARTS\n",
    "    return ...\n",
    "    # STUDENT CODE ENDS\n",
    "\n",
    "# STUDENT CODE STARTS\n",
    "# Use the entropy formula you found in Step 1 above.\n",
    "entropy = ...\n",
    "# STUDENT CODE ENDS\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Entropy of a Bernoulli random variable\")\n",
    "plt.plot(p, entropy)\n",
    "plt.xlabel(\"Probability of heads\")\n",
    "plt.gca().get_xaxis().set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "plt.ylabel(\"Entropy [bits]\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write your solution here\n",
    "\n",
    "**Step 4**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T11:07:36.677016Z",
     "start_time": "2021-02-05T11:07:36.628770Z"
    }
   },
   "source": [
    "## Entropy of a sequence\n",
    "\n",
    "Let's now make the distinction between the empirical entropy -- the average information in a sequence -- with the information content -- how much information there is in total in a sequence. For instance, a fair coin has an entropy of 1 bit, but when tossed 4 times has an information content of 4 bits (a.k.a a nibble).\n",
    "\n",
    "Now implement a function that calculates the entropy of a binary sequence yourself. First, calcultate the parameter p, then plug it in the formula of the entropy (bernoulli)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T09:38:08.945369Z",
     "start_time": "2021-02-09T09:38:08.927559Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_entropy(seq):\n",
    "    \"\"\" Returns the empirical entropy of a sequence.\n",
    "    Input values should contain only 0s and 1s.\"\"\"\n",
    "    # If seq is not a numpy array, it becomes one\n",
    "    if not isinstance(seq, np.ndarray):\n",
    "        seq = np.array(seq)\n",
    "    # STUDENT CODE:\n",
    "    return ...\n",
    "\n",
    "assert binary_entropy([0]) == 0, \"Failed test 1\"\n",
    "assert binary_entropy([0, 1]) == 1, \"Failed test 2\"\n",
    "assert binary_entropy([0]*10 + [1]*10) == 1, \"Failed test 3\"\n",
    "assert np.abs(binary_entropy([0] + [1]*8) - 0.50326) < 1e-5, \"Failed test 4\"\n",
    "assert binary_entropy([]) == 0, \"Failed test 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When the sequence has more than two outcomes\n",
    "\n",
    "Unlike the coin toss, there can be situations where there are more than two outcomes. Rolling a dice is a situation where there are 6 outcomes (assuming the dice has 6 faces). When a fair dice is rolled, one can get either 1 or 2 or 3 or 4 or 5 or 6. Each face has an equal probability to appear. We can still calculate the entropy using the definition introduced at the very beginning of this notebook.\n",
    "\n",
    "Now implement a funtion that calculates the entropy of a random sequence. Make sure you use `xln2x(p)` to calculate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(sequence):\n",
    "    # Assume that sequence is a pandas data frame\n",
    "    entropy = 0.0\n",
    "    # STUDENT CODE STARTS\n",
    "\n",
    "    # STUDENT CODE ENDS\n",
    "    return entropy\n",
    "\n",
    "sequence = pd.DataFrame(data=[1/6 for i in range(6)])\n",
    "assert calculate_entropy(sequence) == 2.584962500721156, \"Failed test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the animal in 20 questions\n",
    "\n",
    "As a kid, you might have travelled by car over long distances and been bored for hours on end and played this game: find the animal/object in so many questions. Perhaps you just were lucky and played the Sims 2 on a portable PlayStation 2. Whatever.\n",
    "\n",
    "This exercise consists of automating the 20 questions game such that it asks 20 questions (or more or less) to find an animal the player has chosen. When you have kids on your own someday (or already), you can entertain your kids for 12+ hours when you travel.\n",
    "\n",
    "You are given a dataset, the columns correspond to features, the rows to names of animals. A given animal has a set of binary features. For instance, a bear has hair, lactate, is a mammal and predator, has teeth, breathes, and so on.\n",
    "\n",
    "You will have to implement an algorithm that creates a binary decision tree and use this tree to create a questionaire (this step is already done for you below) and find an animal in as few guesses as possible.\n",
    "The algorithm to use is ID3, you can refer to the [lecture material](https://user.it.uu.se/~justin/Hugo/courses/machinelearning/lecture8/) to find the algorithm. The measure to use is the entropy and the information gain to choose the top most nodes in the decision tree. The feature with the higher information gain will be closer to the root node. You will have to implement the function `information_gain()` to help with `generate_tree()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T09:38:22.356958Z",
     "start_time": "2021-02-09T09:38:22.232210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download and load the dataset into pandas\n",
    "dataset_url = \"https://raw.githubusercontent.com/earthtojake/20q/master/data/small.csv\"\n",
    "# There is a bigger dataset at\n",
    "# dataset_url = \"https://raw.githubusercontent.com/earthtojake/20q/master/data/big.csv\"\n",
    "df = pd.read_csv(dataset_url, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "The information gain is a measure of how much the entropy of the target variable is reduced after splitting the data based on a feature. It is used to decide the order of features in the decision tree. The information gain is calculated as follows:\n",
    "\n",
    "$$ \\text{Information Gain} = H(Y) - \\sum_{i = 1}^{n} \\frac{|X=r_i|}{|Y|} H(Y|X=r_i) $$\n",
    "\n",
    "Where:\n",
    "- $H(Y)$ is the entropy of the target variable before the split.\n",
    "- $|X=r_i|$ is the number of samples in the dataset where the feature $X$ takes the value $r_i$.\n",
    "- $|Y|$ is the total number of samples in the dataset.\n",
    "- $H(Y|X=r_i)$ is the entropy of the target variable after splitting the data based on the feature $X$ with value $r_i$.\n",
    "\n",
    "> You should use this concept of information gain to implement the function `information_gain(sample, feature)` (block below) that calculates the information gain for a given feature in the dataset. No other approach is allowed to calculate the information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(sample, feature):\n",
    "    # Assume that sample is a pandas data frame\n",
    "    # Assume that feature is some column appearing in sample.\n",
    "    # STUDENT CODE STARTS\n",
    "\n",
    "\n",
    "    \n",
    "    return\n",
    "    # STUDENT CODE ENDS\n",
    "\n",
    "if dataset_url == \"https://raw.githubusercontent.com/earthtojake/20q/master/data/small.csv\":\n",
    "    assert information_gain(df, \"can it fly?\") == 0.9182958340544896, \"Failed test 1\"\n",
    "    assert information_gain(df, \"is it endangered?\") == 0.8112781244591329, \"Failed test 2\"\n",
    "elif dataset_url == \"https://raw.githubusercontent.com/earthtojake/20q/master/data/big.csv\":\n",
    "    assert np.abs(information_gain(df, \"aquatic?\") - 0.9340680553755041) < 1e-5, \"Failed test 1\"\n",
    "    assert np.abs(information_gain(df, \"predator?\") - 0.9927744539878258) < 1e-5, \"Failed test 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T09:38:47.846885Z",
     "start_time": "2021-02-09T09:38:47.774720Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tree = Tree()\n",
    "tree.create_node(\"Root\", \"root\")  # root node\n",
    "\n",
    "def generate_tree(data, tree, branch=\"\", parent=\"root\"):\n",
    "    \"\"\"Populates the tree with questions and candidate animals.\n",
    "    \n",
    "    Args:\n",
    "        data: The dataset to use\n",
    "        tree: The tree to populate\n",
    "        branch: '+' if the previous condition was true, '-' otherwise.\n",
    "        parent: The parent node, so that we can attach children nodes to it.\n",
    "    \n",
    "    Note: Treelib sorts the name of the nodes by alphabetical order, its important\n",
    "    to name the nodes such that the first one is the result of a positive condition,\n",
    "    the second one, the negative condition. This has been taken care of if you use\n",
    "    the code template by naming the node \"+something\" and \"-something\" (In ASCII, +\n",
    "    (42) is before - (45)).\n",
    "    \"\"\"\n",
    "    # STUDENT CODE:\n",
    "    # Find the column with the highest entropy\n",
    "    entropies = ...\n",
    "    # The columns with a null entropy can be removed\n",
    "    ...\n",
    "    # If there aren't any columns left to discriminate the animals, we stop\n",
    "    # If there is only one animal and plenty of columns, we also stop\n",
    "    no_column_left = ...\n",
    "    one_animal_left = ...\n",
    "    if no_column_left or one_animal_left:\n",
    "        tree.create_node(branch+\", \".join(data.index), parent=parent)\n",
    "        return\n",
    "    # If we are here, it means the data can be split some more.\n",
    "    # entropies might have more columns than the current data, since\n",
    "    # we removed some columns.\n",
    "    ...\n",
    "    # We find the column with the highest entropy\n",
    "    ...\n",
    "    # We add the column to the tree under the current parent\n",
    "    node = tree.create_node(branch + selected_column, parent=parent)\n",
    "    # we split the data by column and remove the said column\n",
    "    mask = ...\n",
    "    ...\n",
    "    \n",
    "    # We generate the branches **recursively**\n",
    "    # If the answer to the question was \"yes\"\n",
    "    generate_tree(data[mask], tree, branch=\"+\", parent=node)\n",
    "    # If the answer to the question was \"false\"\n",
    "    generate_tree(data[~mask], tree, branch=\"-\", parent=node)\n",
    "\n",
    "# The tree is generated\n",
    "generate_tree(df, tree)\n",
    "# The tree is plotted\n",
    "print(tree.show(stdout=False))\n",
    "\n",
    "\n",
    "if dataset_url == \"https://raw.githubusercontent.com/earthtojake/20q/master/data/small.csv\":\n",
    "    assert tree.depth() == 5, \"Invalid tree depth\"\n",
    "elif dataset_url == \"https://raw.githubusercontent.com/earthtojake/20q/master/data/big.csv\":\n",
    "    assert tree.depth() == 8, \"Invalid tree depth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it!\n",
    "\n",
    "Just enjoy what you have achieved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T09:39:09.359112Z",
     "start_time": "2021-02-09T09:39:05.416230Z"
    }
   },
   "outputs": [],
   "source": [
    "# No need to add code here!\n",
    "# Just run this and enjoy :)\n",
    "def ask_yesno(question):\n",
    "    \"\"\"Checks that the written answer is valid. Iterates otherwise.\"\"\"\n",
    "    while True:\n",
    "        answer = input(question + \" \").lower().strip()\n",
    "        if answer == \"y\" or answer == \"yes\":\n",
    "            return True\n",
    "        if answer == \"n\" or answer == \"no\":\n",
    "            return False\n",
    "        print(\"Just answer 'yes' or 'no'.\")\n",
    "\n",
    "print(\"Pick an animal!\")\n",
    "print()\n",
    "\n",
    "question = tree.children(\"root\")[0]\n",
    "while True:\n",
    "    # Have we found the answer?\n",
    "    if question.is_leaf():\n",
    "        print(\"I think you chose a \" + question.tag)\n",
    "        break\n",
    "    # Otherwise we keep asking\n",
    "    answer = ask_yesno(question.tag)\n",
    "    children = tree.children(question.identifier)\n",
    "    question = children[0] if answer else children[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
