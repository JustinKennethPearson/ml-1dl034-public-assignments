{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet classification with naive bayes\n",
    "\n",
    "For this notebook we are going to implement a naive bayes classifier for classifying positive or negative based on the words in the tweet. Recall that for two events A and B the bayes theorem says\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "where P(A) and P(B) is the ***class probabilities*** and P(B|A) is called ***conditional probabilities***. this gives us the probability of A happening, given that B has occurred. So as an example if we want to find the probability of \"is this a positive tweet given that it contains the word \"good\" \" we will obtain the following \n",
    "\n",
    "$$ P(\\text{\"positive\"}|\\text{\"good\" in tweet}) = \\frac{P(\"\\text{\"good\" in tweet}|\\text{\"positive\"})P(\\text{\"positive\"})}{P(\"\\text{\"good\" in tweet})} $$\n",
    "\n",
    "This means that to find the probability of \"is this a positive tweet given that it contains the word \"good\" \" we need the probability of \"good\" being in a positive tweet, the probability of a tweet being positive and the probability of \"good\" being in a tweet. \n",
    "\n",
    "Similarly, if we want to obtain the opposite \"is this a negative tweet given that it contains the word \"boring\" \"\n",
    "we get \n",
    "\n",
    "$$ P(\\text{\"negative\"}|\\text{\"boring\" in tweet}) = \\frac{P(\\text{\"boring\" in tweet}|\\text{\"negative\"})P(\\text{\"negative\"})}{P(\\text{\"boring\" in tweet})} $$\n",
    "\n",
    "where we need the probability of \"boring\" being in a negative tweet, the probability of a tweet negative being and the probability of \"boring\" being in a tweet. \n",
    "\n",
    "We can now build a classifier where we compare those two probabilities and whichever is the larger one it's classified as \n",
    "\n",
    "if P(\"positive\"|\"good\" in tweet) $>$ P(\"negative\"|\"boring\" in tweet)\n",
    "    \n",
    "   Tweet is positive\n",
    "\n",
    "else\n",
    "   \n",
    "   Tweet is negative\n",
    "\n",
    "Now let's expand this to handle multiple features and put the Naive assumption into bayes theorem. This means that if features are independent we have \n",
    "\n",
    "$$ P(A,B) = P(A)P(B) $$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{P(b_1|A)P(b_2|A)...P(b_n|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{\\prod_i^nP(b_i|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "\n",
    "So with our previous example expanded with more words \"is this a positive tweet given that it contains the word \"good\" and \"interesting\" \" gives us \n",
    "\n",
    "$$ P(\\text{\"positive\"}|\\text{\"good\", \"interesting\" in tweet}) = \\frac{P(\\text{\"good\" in tweet}|\\text{\"positive\"})P(\\text{\"interesting\" in tweet}|\\text{\"positive\"})P(\\text{\"positive\"})}{P(\\text{\"good\" in tweet})P(\\text{\"interesting\" in tweet})} $$\n",
    "\n",
    "As you can see the denominator remains constant which means we can remove it and the final classifier end up\n",
    "\n",
    "$$y = argmax_A P(A)\\prod_i^nP(b_i|A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that you will be working with can be downloaded from the following link: https://uppsala.instructure.com/courses/66466/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:37:14.983555600Z",
     "start_time": "2024-01-07T13:37:13.641527800Z"
    }
   },
   "outputs": [],
   "source": [
    "#stuff to import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:59:08.449137900Z",
     "start_time": "2024-01-07T13:59:07.974610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hanging out with my friend waiting for a rain ...</td>\n",
       "      <td>anging friend waiting rain band looking huge s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>yesdir... layin not feelin good</td>\n",
       "      <td>esdir layin feelin good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hae a nice night</td>\n",
       "      <td>nice night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Tay...where are you? miss you SO bad</td>\n",
       "      <td>aywhere miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@NeilMcDaid shizer!</td>\n",
       "      <td>neilmcdaid shizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>1</td>\n",
       "      <td>Eating cobb salad w my Patricia at CF.</td>\n",
       "      <td>ating cobb salad patricia compare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>1</td>\n",
       "      <td>@hallowed_ground ever been to fremont? You cou...</td>\n",
       "      <td>hallowedground ever fremont could help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0</td>\n",
       "      <td>@StacyLynn1985 as far as anything to scrape th...</td>\n",
       "      <td>stacylynn1985 anything scrape teeth clue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>1</td>\n",
       "      <td>ksh scripting today to produce html code to di...</td>\n",
       "      <td>scripting today produce html code display pgra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>1</td>\n",
       "      <td>@frelle  Ohhh I've love a Chai too!</td>\n",
       "      <td>frelle ohhh love chai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                              tweet  \\\n",
       "0               0  Hanging out with my friend waiting for a rain ...   \n",
       "1               0                   yesdir... layin not feelin good    \n",
       "2               1                                  Hae a nice night    \n",
       "3               0              Tay...where are you? miss you SO bad    \n",
       "4               0                              @NeilMcDaid shizer!     \n",
       "...           ...                                                ...   \n",
       "199995          1            Eating cobb salad w my Patricia at CF.    \n",
       "199996          1  @hallowed_ground ever been to fremont? You cou...   \n",
       "199997          0  @StacyLynn1985 as far as anything to scrape th...   \n",
       "199998          1  ksh scripting today to produce html code to di...   \n",
       "199999          1               @frelle  Ohhh I've love a Chai too!    \n",
       "\n",
       "                                         processed_tweets  \n",
       "0       anging friend waiting rain band looking huge s...  \n",
       "1                                 esdir layin feelin good  \n",
       "2                                              nice night  \n",
       "3                                            aywhere miss  \n",
       "4                                       neilmcdaid shizer  \n",
       "...                                                   ...  \n",
       "199995                  ating cobb salad patricia compare  \n",
       "199996             hallowedground ever fremont could help  \n",
       "199997           stacylynn1985 anything scrape teeth clue  \n",
       "199998  scripting today produce html code display pgra...  \n",
       "199999                              frelle ohhh love chai  \n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets=pd.read_csv('data_for_theoretical_notebook_1.csv',encoding='latin')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split the data into a training set and a test set using scikit-learns train_test_split function \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:37:59.025504100Z",
     "start_time": "2024-01-07T13:37:59.020482200Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_data = tweets[\"processed_tweets\"]\n",
    "tweets_labels = tweets[\"sentiment\"]\n",
    "\n",
    "#Split data into train_tweets, test_tweets, train_labels and test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to build our classifier is \"probability of positive tweet\" P(pos) , \"probability of negative tweet\" P(neg), \"probability of word in tweet given tweet is positive\" P(w|pos) and \"probability of word in tweet given tweet is negative\" P(w|neg). Start by calculating the probability that a tweet is positive and negative respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pos = ...\n",
    "P_neg = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For P(w|pos), P(w|neg) we need to count how many tweets each word occur in. Count the number of tweets each word occurs in and store in the word counter. An entry in the word counter is for instance {'good': 'Pos':150, 'Neg': 10} meaning good occurs in 150 positive tweets and 10 negative tweets. Be aware that we are not interested in calculating multiple occurrences of the same word in the same tweet. Also, we change the labels from 0 for \"Negative\" and 1 for \"Positive\" to \"Neg\" and \"Pos\" respectively.For each word convert it to lower case. You can use Python's [lower](https://www.w3schools.com/python/ref_string_lower.asp). Another handy Python string method is [split](https://www.w3schools.com/python/ref_string_split.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_labels = train_labels.replace(0, \"Neg\", regex=True)\n",
    "final_train_labels = new_train_labels.replace(1, \"Pos\", regex=True)\n",
    "word_counter = {}\n",
    "for (tweet, label) in zip(train_tweets, final_train_labels):\n",
    "    # ... Count number of tweets each word occurs in and store in word_counter where an entry looks like ex. {'word': 'Pos':98, 'Neg':10}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with a smaller subset of words just to save up some time. Find the 1500 most occuring words in tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_words_to_use = 1500\n",
    "popular_words = sorted(word_counter.items(), key=lambda x: x[1]['Pos'] + x[1]['Neg'], reverse=True)\n",
    "popular_words = [x[0] for x in popular_words[:nr_of_words_to_use]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute P(w|pos), P(w|neg) for the popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_w_given_pos = {}\n",
    "P_w_given_neg = {}\n",
    "for word in popular_words:\n",
    "    #  Calculate the two probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = {\n",
    "    'basis'  : popular_words,\n",
    "    'P(pos)'   : P_pos,\n",
    "    'P(neg)'   : P_neg,\n",
    "    'P(w|pos)' : P_w_given_pos,\n",
    "    'P(w|neg)' : P_w_given_neg\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tweet_classifier function that takes your trained classifier and a tweet and returns wether it's about Positive or Negative using the popular words selected. Note that if there are words in the basis words in our classifier that are not in the tweet we have the opposite probabilities i.e P(w_1 occurs )* P(w_2 does not occur) * .... if w_1 occurs and w_2 does not occur. The function should return wether the tweet is Positive or Negative. i.e 'Pos' or 'Neg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_classifier(tweet, classifier_dict):\n",
    "    \"\"\" param tweet: string containing tweet message\n",
    "        param classifier: dict containing 'basis' - training words\n",
    "                                          'P(pos)' - class probabilities\n",
    "                                          'P(neg)' - class probabilities\n",
    "                                          'P(w|pos)' - conditional probabilities\n",
    "                                          'P(w|neg)' - conditional probabilities\n",
    "        \n",
    "        return: either 'Pos' or 'Neg'\n",
    "    \"\"\"\n",
    "# ... Code for classifying tweets using the naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier, test_tweets, test_labels):\n",
    "    total = len(test_tweets)\n",
    "    correct = 0\n",
    "    for (tweet,label) in zip(test_tweets, test_labels):\n",
    "        predicted = tweet_classifier(tweet,classifier)\n",
    "        if predicted == label:\n",
    "            correct = correct + 1\n",
    "    return(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_labels = test_labels.replace(0, \"Neg\", regex=True)\n",
    "final_test_labels = new_test_labels.replace(1, \"Pos\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_classifier(classifier, test_tweets, final_test_labels)\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional work\n",
    "\n",
    "In basic sentiment analysis classifications we have 3 classes \"Positive\", \"Negative\" and \"Neutral\". Although because it is challenging to create the \"Neutral\" class. Try to improve the accuracy by filtering the dataset from the perspective of removing words that indicate neutrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
